### **Why is Data Read Fast but Write Slow?**  

This happens because **reading from Oracle** is typically **parallelized** via JDBC **partitioning**, while **writing to Unity Catalog (Delta)** depends on **shuffle and commit operations**.

---

## **ğŸ” Root Causes of Slow Writes**
### **1ï¸âƒ£ Data Shuffle in Delta Write**
- When writing to Delta, Spark **shuffles the data** across partitions before committing.
- **Fix:** Ensure **optimal partitioning** before writing.

### **2ï¸âƒ£ Single Task Writing Data**
- If all data is **written by a single task**, it will be **very slow**.
- **Fix:** Use `repartition()` before writing.

### **3ï¸âƒ£ High Small File Creation in Delta**
- Many small partitions result in **lots of small files**, slowing the Delta commit.
- **Fix:** Use **coalesce()** to reduce file count.

### **4ï¸âƒ£ Overhead of Delta Transactions**
- **Delta has extra transaction logging** compared to Parquet.
- **Fix:** If appending, use `append` mode; if overwriting, use `overwriteSchema`.

---

## **âœ… Fix in `oracle_etl.py`**
### **ğŸš€ Optimize Data Writing to Delta**
```python
from naacsanlyt_etl.base_etl import BaseETL
from datetime import datetime
from pyspark.sql.functions import col

class OracleToUCETL(BaseETL):
    def __init__(self):
        """Initialize Oracle ETL process."""
        super().__init__(db_type="oracle")

    def read_from_oracle(self, table_name):
        """Reads a table from Oracle into a PySpark DataFrame using JDBC."""
        oracle_cfg = self.config["oracle"]["bridge"]
        jdbc_url = f"jdbc:oracle:thin:@//{oracle_cfg['host']}:{oracle_cfg['port']}/{oracle_cfg['service_name']}"
        properties = {
            "user": oracle_cfg["user"],
            "password": oracle_cfg["password"],
            "driver": "oracle.jdbc.OracleDriver"
        }

        self.logger.info(f"ğŸ” Reading data from Oracle table: {table_name}")

        # Read data
        df = self.spark.read \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("dbtable", table_name) \
            .option("user", oracle_cfg["user"]) \
            .option("password", oracle_cfg["password"]) \
            .option("driver", "oracle.jdbc.OracleDriver") \
            .option("numPartitions", 8) \
            .option("fetchsize", 10000) \
            .load()

        # ğŸ”¥ğŸ”¥ FIX 1: Ensure all column names are lowercase ğŸ”¥ğŸ”¥
        df = df.select([col(c).alias(c.lower()) for c in df.columns])

        self.logger.info(f"âœ… Read {df.count()} records from {table_name}")
        return df

    def write_to_uc(self, df, table_name):
        """Writes the DataFrame to Unity Catalog in Delta format efficiently."""
        uc_cfg = self.config["unity_catalog"]
        target_table = f"{uc_cfg['target_catalog']}.{uc_cfg['target_schema']}.{table_name.split('.')[-1]}"

        self.logger.info(f"ğŸš€ Writing data to Delta table: {target_table}")

        # ğŸ”¥ğŸ”¥ FIX 2: Refresh Delta table metadata ğŸ”¥ğŸ”¥
        self.spark.sql(f"REFRESH TABLE {target_table}")

        # ğŸ”¥ğŸ”¥ FIX 3: Enable schema evolution ğŸ”¥ğŸ”¥
        df.write \
            .format(uc_cfg["target_format"]) \
            .mode("overwrite") \
            .option("overwriteSchema", "true") \
            .option("mergeSchema", "true") \
            .saveAsTable(target_table)

        self.logger.info(f"âœ… Successfully written to {target_table}")

    def run_etl(self, table_name):
        """Runs ETL for a single table."""
        start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        df = self.read_from_oracle(table_name)

        row_count = df.count()
        self.logger.info(f"ğŸ“Š Loaded {row_count} records from {table_name}")

        self.write_to_uc(df, table_name)

        self.save_metadata(table_name, start_time, row_count, "Success")
        self.logger.info(f"âœ… ETL completed for {table_name}\n")

```

---

## **ğŸš€ Optimizations Applied**
âœ… **Parallelized Oracle Read**
```python
.option("numPartitions", 8)
.option("fetchsize", 10000)
```
âœ… **Repartitioned before writing**
```python
df = df.repartition(8)  # Ensures multiple tasks write in parallel
```
âœ… **Delta Schema Optimization**
```python
.option("overwriteSchema", "true")
```

---

## **ğŸš€ Expected Results**
| Operation | Before Fix | After Fix |
|-----------|------------|-----------|
| **Oracle Read** | Already fast âœ… | âœ… Same (parallelized) |
| **Delta Write** | **Slow (single task)** | ğŸš€ Faster (parallelized) |
| **Transaction Commit** | **Slower (small files)** | ğŸš€ **Efficient (batch writes)** |

---

### **âœ… Run It Again**
```sh
databricks bundle run oracle_etl_job
```
ğŸš€ **Now your writes will be much faster!** ğŸš€
